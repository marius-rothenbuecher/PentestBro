using System.Collections.Generic;
using System.Net;
using System.Text.RegularExpressions;
using PentestBro.Model.Web;

namespace PentestBro.Core.Web
{
    public class RobotsAnalyzer
    {
        private const string RobotsTxtFilePath = "/robots.txt";
        private readonly Regex disallowRegex = new Regex("\\s*Disallow:\\s*([^\\<\\s\r\n]*)");
        private readonly Regex allowRegex = new Regex("\\s*Allow:\\s*([^\\<\\s\r\n]*)");
        private readonly Regex sitemapRegex = new Regex("\\s*Sitemap:\\s*([^\\<\\s\r\n]*)");
        private readonly Browser browser = Browser.Instance;

        public IList<HttpUrl> GetAllowUrls(HttpUrl httpUrl)
        {
            var allowUrls = new List<HttpUrl>();
            var robotsUrl = httpUrl.BaseUrl + RobotsTxtFilePath;
            var response = this.browser.LoadWebsite(robotsUrl, new List<string>());

            if (response.StatusCode == HttpStatusCode.OK)
            {
                var allowMatches = this.allowRegex.Matches(response.Content);
                var allowPaths = this.ExtractPaths(allowMatches);
                allowUrls.AddRange(this.GetHttpUrls(httpUrl, allowPaths));
            }

            return allowUrls;
        }

        public IList<HttpUrl> GetDisallowUrls(HttpUrl httpUrl)
        {
            var disallowUrls = new List<HttpUrl>();
            var robotsUrl = httpUrl.BaseUrl + RobotsTxtFilePath;
            var response = this.browser.LoadWebsite(robotsUrl, new List<string>());

            if (response.StatusCode == HttpStatusCode.OK)
            {
                var disallowMatches = this.disallowRegex.Matches(response.Content);
                var disallowPaths = this.ExtractPaths(disallowMatches);
                disallowUrls.AddRange(this.GetHttpUrls(httpUrl, disallowPaths));
            }

            return disallowUrls;
        }

        public IList<HttpUrl> GetSitemapUrls(HttpUrl httpUrl)
        {
            var sitemapUrls = new List<HttpUrl>();
            var robotsUrl = httpUrl.BaseUrl + RobotsTxtFilePath;
            var response = this.browser.LoadWebsite(robotsUrl, new List<string>());

            if (response.StatusCode == HttpStatusCode.OK)
            {
                var sitemapMatches = this.sitemapRegex.Matches(response.Content);
                var sitemapPaths = this.ExtractPaths(sitemapMatches);
                sitemapUrls.AddRange(this.GetHttpUrls(httpUrl, sitemapPaths));
            }

            return sitemapUrls;
        }

        private IList<HttpUrl> GetHttpUrls(HttpUrl sourceHttpUrl, IList<string> paths)
        {
            var httpUrls = new List<HttpUrl>();

            foreach (var path in paths)
            {
                var hrefType = UrlParser.GetHrefType(path);

                if (hrefType == HrefType.FullUrl)
                {
                    var httpUrl = UrlParser.GetHttpUrl(path);
                    httpUrls.Add(httpUrl);
                }
                else if (hrefType == HrefType.RelativePath)
                {
                    var httpUrl = UrlParser.GetHttpUrl(sourceHttpUrl, path);
                    httpUrls.Add(httpUrl);
                }
            }

            return httpUrls;
        }

        private IList<string> ExtractPaths(MatchCollection matches)
        {
            var paths = new List<string>();

            foreach (Match match in matches)
            {
                if (match.Groups.Count == 2)
                {
                    var path = match.Groups[1].Value;
                    path = path.Replace("\r", string.Empty);
                    path = path.Replace("\n", string.Empty);

                    if (paths.Contains(path) == false)
                        paths.Add(path);
                }
            }

            return paths;
        }
    }
}
